import asyncio
import json
import os
from typing import Dict, Any

from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_neo4j import Neo4jGraph, GraphCypherQAChain

from backend.core.graph_client import langchain_llm
from backend.services.search_service import SearchService


class ChatService:
    """
    é«˜ç²¾åº¦é—®ç­”æœåŠ¡ï¼šå¹¶è¡Œæ‰§è¡Œå‘é‡æ£€ç´¢å’Œå›¾è°±ç»Ÿè®¡ï¼Œç”± LLM äº¤å‰éªŒè¯
    """

    # åˆå§‹åŒ–ä¸€ä¸ªç‹¬ç«‹çš„ Neo4jGraph ç”¨äº LangChain Chain
    _graph = Neo4jGraph(
        url=os.getenv("NEO4J_URI"),
        username=os.getenv("NEO4J_USERNAME"),
        password=os.getenv("NEO4J_PASSWORD")
    )

    # å®šä¹‰ Cypher ç”Ÿæˆè§„åˆ™
    _cypher_prompt = PromptTemplate(
        template="""
        Task: Generate Cypher query for Neo4j.
        Schema: {schema}
        Instructions:
        1. Nodes: `Product`, `ProductAttr`, `StandardDocument`.
        2. Edge: 
           - (Product)-[:HAS_ATTR]->(ProductAttr)
           - (StandardDocument)-[:APPLIES_TO]->(Product) 
        3. Do NOT query `Observation` or `Episode`.
        4. For aggregation/counting, use count().
        5. For string matching, use CONTAINS.

        Question: {question}
        """,
        input_variables=["schema", "question"]
    )

    _cypher_chain = GraphCypherQAChain.from_llm(
        llm=langchain_llm,
        graph=_graph,
        cypher_prompt=_cypher_prompt,
        verbose=True,
        return_direct=True,  # ç›´æ¥è¿”å›æ•°æ®ç»“æœï¼Œä¸è®© Chain è‡ªåŠ¨å›ç­”
        allow_dangerous_requests=True
    )

    @staticmethod
    async def run_deep_accuracy_chat(question: str) -> Dict[str, Any]:
        print(f"ğŸ§  [DeepMode] å¯åŠ¨é«˜ç²¾åº¦åŒè·¯æ£€ç´¢: {question}")

        # --- å¹¶è¡Œæ‰§è¡Œï¼šè·¯A (å‘é‡) + è·¯B (ç»Ÿè®¡) ---
        task_vector = SearchService.hybrid_search(question, limit=5)
        task_graph = ChatService._safe_cypher_run(question)

        # ç­‰å¾…ç»“æœ
        vector_res, graph_res = await asyncio.gather(task_vector, task_graph)

        # --- ç»“æœèåˆ ---
        context_str = "ã€æ¥æº1ï¼šè¯­ä¹‰æ£€ç´¢ (Qdrant)ã€‘\n"
        if vector_res.get("results"):
            for item in vector_res["results"]:
                info = {
                    "äº§å“ç¼–ç ": item["code"],
                    "å±æ€§": item["graph_data"],
                    "æè¿°": item["semantic_text"][:100]
                }
                context_str += f"- {json.dumps(info, ensure_ascii=False, default=str)}\n"
        else:
            context_str += "(æ— ç›¸å…³ç»“æœ)\n"

        context_str += "\nã€æ¥æº2ï¼šå…¨åº“ç»Ÿè®¡ (Neo4j Cypher)ã€‘\n"
        if graph_res:
            context_str += f"{json.dumps(graph_res, ensure_ascii=False, default=str)}\n"
        else:
            context_str += "(æ— ç»Ÿè®¡æ•°æ®)\n"

        # --- æœ€ç»ˆæ¨ç† ---
        final_answer = await ChatService._synthesize_answer(question, context_str)

        return {
            "answer": final_answer,
            "sources": {
                "vector": [x["code"] for x in vector_res.get("results", [])],
                "graph_query": str(graph_res)[:200]
            }
        }

    @staticmethod
    async def _safe_cypher_run(question: str):
        """å®‰å…¨è¿è¡Œ Cypherï¼Œå¤±è´¥åˆ™è¿”å› None"""
        try:
            return await ChatService._cypher_chain.ainvoke({"query": question})
        except Exception as e:
            print(f"âš ï¸ Cypher å¤±è´¥ (éç»Ÿè®¡ç±»é—®é¢˜å¯å¿½ç•¥): {e}")
            return None

    @staticmethod
    async def _synthesize_answer(question: str, context: str) -> str:
        prompt = ChatPromptTemplate.from_messages([
            ("system", "ä½ æ˜¯ä¸€ä¸ªä¸¥è°¨çš„æ•°æ®åˆ†æå¸ˆã€‚è¯·æ ¹æ®æä¾›çš„ä¸¤ä»½æ•°æ®æ¥æºå›ç­”ç”¨æˆ·é—®é¢˜ã€‚\n"
                       "è§„åˆ™ï¼š\n"
                       "1. å¦‚æœæ˜¯ç»Ÿè®¡é—®é¢˜(å¤šå°‘ä¸ª/æ€»å…±)ï¼Œä¼˜å…ˆä¿¡ä»»ã€æ¥æº2ã€‘ã€‚\n"
                       "2. å¦‚æœæ˜¯è¯¦æƒ…æˆ–æ¨èé—®é¢˜ï¼Œä¼˜å…ˆä¿¡ä»»ã€æ¥æº1ã€‘ã€‚\n"
                       "3. å¦‚æœéƒ½æ²¡æœ‰æ•°æ®ï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚"),
            ("human", "è¯æ®æ•°æ®:\n{context}\n\né—®é¢˜: {question}")
        ])
        chain = prompt | langchain_llm | StrOutputParser()
        return await chain.ainvoke({"context": context, "question": question})
